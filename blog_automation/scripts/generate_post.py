"""
seasonal_top_anime.jsonì„ ì½ì–´ ê° ì• ë‹ˆë§ˆë‹¤ í•œêµ­ì–´ ë¸”ë¡œê·¸ ê¸€ ì´ˆì•ˆ ìƒì„± (Claude API â†’ Gemini fallback)
ì»¤ë²„ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ â†’ output/images/
ê¸€ ì €ì¥ â†’ output/posts/ì• ë‹ˆì œëª©.md

ìˆ˜ì • ëª¨ë“œ: --revise <mdíŒŒì¼ê²½ë¡œ> --instruction <ì§€ì‹œë¬¸>
  â†’ í•´ë‹¹ .md íŒŒì¼ ë‚´ìš©ì„ ì§€ì‹œì— ë§ê²Œ Claudeë¡œ ìˆ˜ì • í›„ ê°™ì€ íŒŒì¼ì— ë®ì–´ì“°ê¸°.
  â†’ atlas_bot.py ë“±ì—ì„œ subprocessë¡œ í˜¸ì¶œ ì‹œ ì‚¬ìš©.

LLM ì „ëµ:
  1ì°¨: Claude Sonnet (ê³ í’ˆì§ˆ)
  2ì°¨: Gemini 2.5 Flash fallback (Claude rate limit ë˜ëŠ” ì˜¤ë¥˜ ì‹œ ìë™ ì „í™˜)
  í™˜ê²½ë³€ìˆ˜: ANTHROPIC_API_KEY, GOOGLE_API_KEY

í™•ì¥ ë°ì´í„° ì†ŒìŠ¤:
  - AniList GraphQL: ì• ë‹ˆ ê¸°ë³¸ ì •ë³´ + ìºë¦­í„°/ì„±ìš° + ê´€ë ¨ ì‘í’ˆ
  - TMDB API: í¬ìŠ¤í„°/ìŠ¤í‹¸ì»· ì´ë¯¸ì§€ + íŠ¸ë ˆì¼ëŸ¬ ì •ë³´ + ì¶”ê°€ ì¤„ê±°ë¦¬
  - YouTube Data API: ê³µì‹ PV/íŠ¸ë ˆì¼ëŸ¬ ë§í¬ + ì‹œì²­ì ë°˜ì‘
  - Reddit API: íŒ¬ ë°˜ì‘/í™”ì œ ëŒ“ê¸€ (ì¸ê¸° ì„œë¸Œë ˆë”§)
  í™˜ê²½ë³€ìˆ˜: TMDB_API_KEY, YOUTUBE_API_KEY, REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET
"""

import argparse
import json
import os
import re
import sys
import time
import urllib.parse
import urllib.request
from pathlib import Path

import requests
from anthropic import Anthropic
from dotenv import load_dotenv

load_dotenv()

SCRIPT_DIR = Path(__file__).resolve().parent
OUTPUT_DIR = SCRIPT_DIR.parent.parent / "output"
INPUT_JSON = OUTPUT_DIR / "seasonal_top_anime.json"
IMAGES_DIR = OUTPUT_DIR / "images"
POSTS_DIR = OUTPUT_DIR / "posts"

SEASON_KR = {"WINTER": "ê²¨ìš¸", "SPRING": "ë´„", "SUMMER": "ì—¬ë¦„", "FALL": "ê°€ì„"}

# TMDB ì´ë¯¸ì§€ ë² ì´ìŠ¤ URL
TMDB_IMAGE_BASE = "https://image.tmdb.org/t/p/w780"
TMDB_IMAGE_ORIGINAL = "https://image.tmdb.org/t/p/original"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸ë¦¬í‹°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def slugify(text: str, max_len: int = 80) -> str:
    """íŒŒì¼ëª…/URLìš© ìŠ¬ëŸ¬ê·¸ ìƒì„±."""
    text = re.sub(r"[^\w\s\-]", "", text)
    text = re.sub(r"[-\s]+", "-", text).strip("-")
    return text[:max_len] or "untitled"


def download_image(url: str, save_path: Path) -> bool:
    """ì´ë¯¸ì§€ë¥¼ save_pathì— ë‹¤ìš´ë¡œë“œ. ì„±ê³µ ì‹œ True ë°˜í™˜."""
    try:
        resp = requests.get(url, timeout=30)
        resp.raise_for_status()
        save_path.parent.mkdir(parents=True, exist_ok=True)
        save_path.write_bytes(resp.content)
        return True
    except Exception as e:
        print(f"  âš ï¸  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({url}): {e}")
        return False


def get_image_extension(url: str) -> str:
    """URLì—ì„œ í™•ì¥ì ì¶”ì¶œ, ì—†ìœ¼ë©´ .jpg."""
    path = urllib.parse.urlparse(url).path
    ext = Path(path).suffix.lower()
    return ext if ext in (".jpg", ".jpeg", ".png", ".webp", ".gif") else ".jpg"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TMDB API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def tmdb_search_anime(title_en: str, title_native: str = None) -> dict:
    """TMDBì—ì„œ ì• ë‹ˆë©”ì´ì…˜ ê²€ìƒ‰ â†’ ìƒì„¸ ì •ë³´ ë°˜í™˜."""
    api_key = os.environ.get("TMDB_API_KEY")
    if not api_key:
        return {}

    search_titles = [t for t in [title_en, title_native] if t]
    for search_query in search_titles:
        try:
            # TV ì‹œë¦¬ì¦ˆ ê²€ìƒ‰ (ì• ë‹ˆëŠ” ëŒ€ë¶€ë¶„ TV)
            url = (
                f"https://api.themoviedb.org/3/search/tv"
                f"?api_key={api_key}&query={urllib.parse.quote(search_query)}&language=ko-KR"
            )
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
            results = resp.json().get("results", [])
            if not results:
                continue

            # ì²« ë²ˆì§¸ ê²°ê³¼ ìƒì„¸ ì¡°íšŒ
            tmdb_id = results[0]["id"]
            detail_url = (
                f"https://api.themoviedb.org/3/tv/{tmdb_id}"
                f"?api_key={api_key}&language=ko-KR&append_to_response=images,videos"
            )
            detail_resp = requests.get(detail_url, timeout=10)
            detail_resp.raise_for_status()
            detail = detail_resp.json()

            # ì´ë¯¸ì§€ ìˆ˜ì§‘ (í¬ìŠ¤í„° + ë°±ë“œë¡­)
            images_data = detail.get("images", {})
            posters = images_data.get("posters", [])[:5]
            backdrops = images_data.get("backdrops", [])[:5]

            # íŠ¸ë ˆì¼ëŸ¬ ìˆ˜ì§‘
            videos = detail.get("videos", {}).get("results", [])
            trailers = [v for v in videos if v.get("type") in ("Trailer", "Teaser")][:3]

            return {
                "tmdb_id": tmdb_id,
                "overview_ko": detail.get("overview", ""),
                "vote_average": detail.get("vote_average", 0),
                "vote_count": detail.get("vote_count", 0),
                "first_air_date": detail.get("first_air_date", ""),
                "networks": [n.get("name") for n in detail.get("networks", [])],
                "poster_path": detail.get("poster_path", ""),
                "backdrop_paths": [b["file_path"] for b in backdrops if b.get("file_path")],
                "poster_paths": [p["file_path"] for p in posters if p.get("file_path")],
                "trailers": [
                    {"key": t["key"], "name": t["name"], "site": t["site"]}
                    for t in trailers
                ],
            }
        except Exception as e:
            print(f"  âš ï¸  TMDB ê²€ìƒ‰ ì‹¤íŒ¨ ({search_query}): {e}")
            continue
    return {}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AniList ì¶”ê°€ ë°ì´í„° (ìºë¦­í„°, ì„±ìš°, ê´€ë ¨ ì‘í’ˆ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def anilist_get_details(anime_id: int) -> dict:
    """AniList GraphQLë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ (ìºë¦­í„°, ì„±ìš°, ìŠ¤íƒœí”„, ê´€ë ¨ ì‘í’ˆ)."""
    query = """
    query ($id: Int) {
      Media(id: $id, type: ANIME) {
        id
        title { romaji english native }
        studios(isMain: true) { nodes { name } }
        staff(perPage: 5, sort: [RELEVANCE]) {
          nodes {
            name { full native }
            primaryOccupations
          }
        }
        characters(perPage: 6, sort: [ROLE, RELEVANCE]) {
          nodes {
            name { full native }
            description
            image { medium }
          }
          edges {
            role
            voiceActors(language: JAPANESE) {
              name { full native }
            }
          }
        }
        relations {
          nodes {
            id title { romaji english }
            type format status
          }
          edges { relationType }
        }
        recommendations(perPage: 3) {
          nodes {
            mediaRecommendation {
              title { romaji english }
              averageScore
            }
          }
        }
        tags { name rank isMediaSpoiler }
        trailer { id site }
        externalLinks { url site }
      }
    }
    """
    try:
        resp = requests.post(
            "https://graphql.anilist.co",
            json={"query": query, "variables": {"id": anime_id}},
            headers={"Content-Type": "application/json"},
            timeout=15,
        )
        resp.raise_for_status()
        data = resp.json()
        media = data.get("data", {}).get("Media", {})
        if not media:
            return {}

        # ìŠ¤íŠœë””ì˜¤
        studios = [s["name"] for s in media.get("studios", {}).get("nodes", [])]

        # ìŠ¤íƒœí”„ (ê°ë… ë“±)
        staff = []
        for s in media.get("staff", {}).get("nodes", []):
            staff.append({
                "name": s.get("name", {}).get("full", ""),
                "role": ", ".join(s.get("primaryOccupations", [])[:2]),
            })

        # ìºë¦­í„° & ì„±ìš°
        char_nodes = media.get("characters", {}).get("nodes", [])
        char_edges = media.get("characters", {}).get("edges", [])
        characters = []
        for node, edge in zip(char_nodes, char_edges):
            va_list = edge.get("voiceActors", [])
            va_name = va_list[0]["name"]["full"] if va_list else ""
            characters.append({
                "name": node.get("name", {}).get("full", ""),
                "name_native": node.get("name", {}).get("native", ""),
                "role": edge.get("role", ""),
                "voice_actor": va_name,
                "image": node.get("image", {}).get("medium", ""),
            })

        # ê´€ë ¨ ì‘í’ˆ
        rel_nodes = media.get("relations", {}).get("nodes", [])
        rel_edges = media.get("relations", {}).get("edges", [])
        relations = []
        for node, edge in zip(rel_nodes, rel_edges):
            relations.append({
                "title": node.get("title", {}).get("romaji", ""),
                "relation": edge.get("relationType", ""),
                "format": node.get("format", ""),
            })

        # ì¶”ì²œ ì‘í’ˆ
        recs = []
        for r in media.get("recommendations", {}).get("nodes", []):
            mr = r.get("mediaRecommendation", {})
            if mr:
                recs.append({
                    "title": mr.get("title", {}).get("romaji", ""),
                    "score": mr.get("averageScore", 0),
                })

        # íƒœê·¸ (ìŠ¤í¬ì¼ëŸ¬ ì œì™¸, ìƒìœ„ 8ê°œ)
        tags = [
            t["name"] for t in media.get("tags", [])
            if not t.get("isMediaSpoiler") and t.get("rank", 0) >= 60
        ][:8]

        # íŠ¸ë ˆì¼ëŸ¬
        trailer = media.get("trailer")
        trailer_url = ""
        if trailer:
            if trailer.get("site") == "youtube":
                trailer_url = f"https://www.youtube.com/watch?v={trailer['id']}"

        # ì™¸ë¶€ ë§í¬
        ext_links = {
            link["site"]: link["url"]
            for link in media.get("externalLinks", [])
            if link.get("site") in ("Crunchyroll", "Netflix", "Amazon Prime Video", "Funimation", "Bilibili")
        }

        return {
            "studios": studios,
            "staff": staff,
            "characters": characters,
            "relations": relations,
            "recommendations": recs,
            "tags": tags,
            "trailer_url": trailer_url,
            "streaming": ext_links,
        }
    except Exception as e:
        print(f"  âš ï¸  AniList ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# YouTube Data API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def youtube_search_pv(title_en: str, title_native: str = None) -> list[dict]:
    """YouTubeì—ì„œ ê³µì‹ PV/íŠ¸ë ˆì¼ëŸ¬ ê²€ìƒ‰."""
    api_key = os.environ.get("YOUTUBE_API_KEY")
    if not api_key:
        return []

    search_queries = []
    if title_en:
        search_queries.append(f"{title_en} official trailer PV")
    if title_native:
        search_queries.append(f"{title_native} PV å…¬å¼")

    results = []
    for query in search_queries[:1]:  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ë§Œ ì‚¬ìš© (API ì¿¼í„° ì ˆì•½)
        try:
            url = (
                f"https://www.googleapis.com/youtube/v3/search"
                f"?key={api_key}&q={urllib.parse.quote(query)}&part=snippet"
                f"&type=video&maxResults=3&order=relevance&videoDuration=short"
            )
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
            items = resp.json().get("items", [])
            for item in items:
                vid_id = item.get("id", {}).get("videoId", "")
                snippet = item.get("snippet", {})
                if vid_id:
                    results.append({
                        "video_id": vid_id,
                        "title": snippet.get("title", ""),
                        "channel": snippet.get("channelTitle", ""),
                        "url": f"https://www.youtube.com/watch?v={vid_id}",
                        "thumbnail": snippet.get("thumbnails", {}).get("high", {}).get("url", ""),
                    })
            if results:
                break
        except Exception as e:
            print(f"  âš ï¸  YouTube ê²€ìƒ‰ ì‹¤íŒ¨ ({query}): {e}")
    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Reddit API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def reddit_get_discussions(title_en: str, title_native: str = None) -> list[dict]:
    """Redditì—ì„œ í•´ë‹¹ ì• ë‹ˆë©”ì´ì…˜ ê´€ë ¨ ì¸ê¸° ê¸€ ìˆ˜ì§‘."""
    client_id = os.environ.get("REDDIT_CLIENT_ID")
    client_secret = os.environ.get("REDDIT_CLIENT_SECRET")

    subreddits = ["anime", "Animesuggest", "anime_titties"]  # ì£¼ìš” ì• ë‹ˆ ì„œë¸Œë ˆë”§

    headers = {"User-Agent": "GeekBrox/1.0 (blog automation)"}

    # ì¸ì¦ ì—†ì´ ê³µê°œ API ì‚¬ìš© (read-only)
    results = []
    search_query = title_en or title_native or ""
    if not search_query:
        return []

    try:
        # r/anime ì—ì„œ ê²€ìƒ‰
        url = (
            f"https://www.reddit.com/r/anime/search.json"
            f"?q={urllib.parse.quote(search_query)}&sort=top&limit=5&t=year&restrict_sr=1"
        )
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        posts = resp.json().get("data", {}).get("children", [])
        for post in posts[:3]:
            data = post.get("data", {})
            score = data.get("score", 0)
            if score < 100:  # ìµœì†Œ upvote í•„í„°
                continue
            results.append({
                "title": data.get("title", ""),
                "score": score,
                "url": f"https://reddit.com{data.get('permalink', '')}",
                "num_comments": data.get("num_comments", 0),
                "selftext": data.get("selftext", "")[:300],  # ë³¸ë¬¸ 300ì
            })
    except Exception as e:
        print(f"  âš ï¸  Reddit ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©€í‹°ì†ŒìŠ¤ ì´ë¯¸ì§€ ìˆ˜ì§‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def collect_images(anime: dict, tmdb_data: dict, anilist_details: dict, slug: str) -> dict:
    """
    ì´ 5ê°œ ì´ë¯¸ì§€ ìˆ˜ì§‘:
    1. ì»¤ë²„ ì´ë¯¸ì§€ (AniList, ê¸€ ìƒë‹¨)
    2. TMDB í¬ìŠ¤í„° (ê¸°ë³¸ ì •ë³´ ì„¹ì…˜)
    3. TMDB ìŠ¤í‹¸ì»· 1 (ìŠ¤í† ë¦¬ ì„¹ì…˜)
    4. TMDB ìŠ¤í‹¸ì»· 2 (ë³¼ê±°ë¦¬ ì„¹ì…˜)
    5. TMDB í¬ìŠ¤í„° or AniList ì»¤ë²„ ë³€í˜• (ë§ˆë¬´ë¦¬ ì§ì „)
    ë°˜í™˜: {image_key: relative_path}
    """
    IMAGES_DIR.mkdir(parents=True, exist_ok=True)
    paths = {}

    # 1. ì»¤ë²„ ì´ë¯¸ì§€ (AniList)
    cover_url = anime.get("cover_image_url", "")
    if cover_url:
        ext = get_image_extension(cover_url)
        cover_path = IMAGES_DIR / f"{slug}_cover{ext}"
        if download_image(cover_url, cover_path):
            paths["cover"] = f"../images/{slug}_cover{ext}"

    # TMDB ì´ë¯¸ì§€ ì²˜ë¦¬
    backdrop_paths = tmdb_data.get("backdrop_paths", [])
    poster_paths = tmdb_data.get("poster_paths", [])

    # 2. TMDB í¬ìŠ¤í„° (ê¸°ë³¸ ì •ë³´ìš©)
    if poster_paths:
        poster_url = TMDB_IMAGE_BASE + poster_paths[0]
        poster_path = IMAGES_DIR / f"{slug}_poster.jpg"
        if download_image(poster_url, poster_path):
            paths["poster"] = f"../images/{slug}_poster.jpg"
    elif cover_url:
        # TMDB ì—†ìœ¼ë©´ AniList ì»¤ë²„ ì¬ì‚¬ìš©
        paths["poster"] = paths.get("cover", "")

    # 3. TMDB ìŠ¤í‹¸ì»· 1 (ìŠ¤í† ë¦¬ìš©)
    if len(backdrop_paths) >= 1:
        still1_url = TMDB_IMAGE_BASE + backdrop_paths[0]
        still1_path = IMAGES_DIR / f"{slug}_still1.jpg"
        if download_image(still1_url, still1_path):
            paths["still1"] = f"../images/{slug}_still1.jpg"

    # 4. TMDB ìŠ¤í‹¸ì»· 2 (ë³¼ê±°ë¦¬ìš©)
    if len(backdrop_paths) >= 2:
        still2_url = TMDB_IMAGE_BASE + backdrop_paths[1]
        still2_path = IMAGES_DIR / f"{slug}_still2.jpg"
        if download_image(still2_url, still2_path):
            paths["still2"] = f"../images/{slug}_still2.jpg"
    elif len(backdrop_paths) == 1:
        paths["still2"] = paths.get("still1", "")

    # 5. ë§ˆë¬´ë¦¬ ì§ì „ ì´ë¯¸ì§€ (TMDB 3ë²ˆì§¸ ìŠ¤í‹¸ì»· or ëŒ€ì²´)
    if len(backdrop_paths) >= 3:
        still3_url = TMDB_IMAGE_BASE + backdrop_paths[2]
        still3_path = IMAGES_DIR / f"{slug}_still3.jpg"
        if download_image(still3_url, still3_path):
            paths["still3"] = f"../images/{slug}_still3.jpg"
    elif poster_paths and len(poster_paths) >= 2:
        alt_poster_url = TMDB_IMAGE_BASE + poster_paths[1]
        alt_path = IMAGES_DIR / f"{slug}_poster2.jpg"
        if download_image(alt_poster_url, alt_path):
            paths["still3"] = f"../images/{slug}_poster2.jpg"
    else:
        paths["still3"] = paths.get("poster", paths.get("cover", ""))

    return paths


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _is_rate_limit_error(e: Exception) -> bool:
    msg = str(e).lower()
    return any(kw in msg for kw in ("rate_limit", "rate limit", "429", "too many requests", "overloaded"))


def _call_gemini(prompt: str, max_tokens: int = 8192) -> str:
    gemini_key = os.environ.get("GOOGLE_API_KEY")
    if not gemini_key:
        raise RuntimeError(
            "GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. .envì— ì¶”ê°€í•˜ê±°ë‚˜ Google AI Studioì—ì„œ ë°œê¸‰í•˜ì„¸ìš”.\n"
            "ë°œê¸‰: https://aistudio.google.com/apikey"
        )
    try:
        from google import genai
        from google.genai import types
        client = genai.Client(api_key=gemini_key)
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config=types.GenerateContentConfig(max_output_tokens=max_tokens),
        )
        return response.text
    except Exception as e:
        raise RuntimeError(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}") from e


def _call_llm(prompt: str, max_tokens: int = 8192) -> str:
    """Claude â†’ Gemini fallback."""
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if api_key:
        try:
            client = Anthropic(api_key=api_key)
            message = client.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}],
            )
            block = message.content[0]
            if block.type != "text":
                raise RuntimeError(f"Claude API ë¹„í…ìŠ¤íŠ¸ ì‘ë‹µ: {block.type}")
            return block.text
        except Exception as e:
            if _is_rate_limit_error(e):
                print("  âš ï¸  Claude rate limit â†’ Gemini fallbackìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            else:
                raise RuntimeError(f"Claude API í˜¸ì¶œ ì‹¤íŒ¨: {e}") from e
    else:
        print("  âš ï¸  ANTHROPIC_API_KEY ì—†ìŒ â†’ Gemini fallbackìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

    print("  ğŸ¤– Gemini 2.5 Flash í˜¸ì¶œ ì¤‘...")
    text = _call_gemini(prompt, max_tokens=max_tokens)
    print("  âœ… Gemini fallback ì„±ê³µ")
    return text


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¸”ë¡œê·¸ ê¸€ ìƒì„± (í™•ì¥ ë²„ì „)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_blog_draft(
    anime: dict,
    season_label: str,
    image_paths: dict,
    anilist_details: dict,
    tmdb_data: dict,
    youtube_data: list,
    reddit_data: list,
) -> str:
    """
    ë‹¤ì¤‘ API ë°ì´í„°ë¥¼ í†µí•©í•œ ê³ í’ˆì§ˆ í•œêµ­ì–´ ë¸”ë¡œê·¸ ê¸€ ìƒì„±.
    ì´ë¯¸ì§€ 5ê°œ ì‚½ì… êµ¬ì¡°:
      - ê¸€ ìƒë‹¨: cover
      - ê¸°ë³¸ ì •ë³´ ì§í›„: poster
      - ìŠ¤í† ë¦¬ ì†Œê°œ ì§í›„: still1
      - ë³¼ê±°ë¦¬/í¬ì¸íŠ¸ ì§í›„: still2
      - ì´í‰ ì§ì „: still3
    """
    title_display = (
        anime.get("title_korean")
        or anime.get("title_english")
        or anime.get("title_native")
        or "ì œëª© ì—†ìŒ"
    )
    title_en = anime.get("title_english") or anime.get("title_native") or ""
    post_title = f"[{season_label} ì• ë‹ˆ] {title_display} - ì •ë³´ & ë¦¬ë·°"

    # ì´ë¯¸ì§€ ê²½ë¡œ
    img_cover  = image_paths.get("cover", "")
    img_poster = image_paths.get("poster", "")
    img_still1 = image_paths.get("still1", "")
    img_still2 = image_paths.get("still2", "")
    img_still3 = image_paths.get("still3", "")

    # AniList ì¶”ê°€ ì •ë³´ í¬ë§·
    studios_str  = ", ".join(anilist_details.get("studios", [])) or "ì •ë³´ ì—†ìŒ"
    staff_str = "\n".join(
        f"- {s['name']} ({s['role']})" for s in anilist_details.get("staff", [])
    ) or "ì •ë³´ ì—†ìŒ"
    chars_str = "\n".join(
        f"- {c['name']} ({c['name_native']}) â€” CV: {c['voice_actor']} [{c['role']}]"
        for c in anilist_details.get("characters", [])
    ) or "ì •ë³´ ì—†ìŒ"
    tags_str = ", ".join(anilist_details.get("tags", [])) or ""
    trailer_url = anilist_details.get("trailer_url", "")
    streaming_str = "\n".join(
        f"- {site}: {url}" for site, url in anilist_details.get("streaming", {}).items()
    ) or ""
    recs_str = "\n".join(
        f"- {r['title']} (AniList {r['score']}/100)"
        for r in anilist_details.get("recommendations", [])
    ) or ""

    # TMDB ì¶”ê°€ ì •ë³´
    tmdb_overview  = tmdb_data.get("overview_ko", "")
    tmdb_vote      = tmdb_data.get("vote_average", 0)
    tmdb_vote_cnt  = tmdb_data.get("vote_count", 0)
    tmdb_air_date  = tmdb_data.get("first_air_date", "")
    tmdb_networks  = ", ".join(tmdb_data.get("networks", [])) or ""
    tmdb_trailers  = tmdb_data.get("trailers", [])
    trailer_yt_str = ""
    if tmdb_trailers:
        for t in tmdb_trailers:
            if t.get("site") == "YouTube":
                trailer_yt_str = f"https://www.youtube.com/watch?v={t['key']}"
                break

    # YouTube PV ì •ë³´
    yt_pv_str = ""
    if youtube_data:
        pv = youtube_data[0]
        yt_pv_str = f"- [{pv['title']}]({pv['url']}) (ì±„ë„: {pv['channel']})"

    # Reddit ë°˜ì‘
    reddit_str = ""
    if reddit_data:
        reddit_str = "\n".join(
            f"- r/anime ì¸ê¸° ê¸€: \"{post['title']}\" (ğŸ‘ {post['score']}, ğŸ’¬ {post['num_comments']}ê°œ ëŒ“ê¸€)"
            for post in reddit_data[:2]
        )

    # ì´ë¯¸ì§€ ë§ˆí¬ë‹¤ìš´ í—¬í¼
    def img_md(path: str, alt: str) -> str:
        if not path:
            return ""
        return f"![{alt}]({path})\n\n"

    # ì´ë¯¸ì§€ ê²½ë¡œ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬
    cover_md  = img_md(img_cover,  "ì»¤ë²„ ì´ë¯¸ì§€")
    poster_md = img_md(img_poster, f"{title_display} í¬ìŠ¤í„°")
    still1_md = img_md(img_still1, f"{title_display} ìŠ¤í‹¸ì»· 1")
    still2_md = img_md(img_still2, f"{title_display} ìŠ¤í‹¸ì»· 2")
    still3_md = img_md(img_still3, f"{title_display} ìŠ¤í‹¸ì»· 3")

    prompt = f"""ë‹¤ìŒ ì• ë‹ˆë©”ì´ì…˜ì— ëŒ€í•œ **ì‹¬ì¸µ í•œêµ­ì–´ ë¸”ë¡œê·¸ ê¸€**ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
ê¸°ì¡´ ë‹¨ìˆœ ì†Œê°œ ê¸€ë³´ë‹¤ 3~4ë°° ë§ì€ ë¶„ëŸ‰ìœ¼ë¡œ, íŒ¬ë“¤ì´ ì •ë§ ì½ê³  ì‹¶ì–´ í•˜ëŠ” ê¹Šì´ ìˆëŠ” ì •ë³´ì™€ ë¶„ì„ì„ ë‹´ì•„ì•¼ í•©ë‹ˆë‹¤.

---

## ë¸”ë¡œê·¸ ê¸€ í˜•ì‹ & ì´ë¯¸ì§€ ë°°ì¹˜ ê·œì¹™

**ì´ë¯¸ì§€ëŠ” ë°˜ë“œì‹œ ì•„ë˜ ìˆœì„œì™€ ìœ„ì¹˜ì— ì •í™•íˆ ì‚½ì…í•˜ì„¸ìš” (ê²½ë¡œ ë³€ê²½ ê¸ˆì§€):**

1. **ê¸€ ë§¨ ì²« ì¤„**: `# ì œëª©` ë°”ë¡œ ë‹¤ìŒ ì¤„
{cover_md if cover_md else "(ì»¤ë²„ ì´ë¯¸ì§€ ì—†ìŒ)"}

2. **ê¸°ë³¸ ì •ë³´ ì„¹ì…˜ ì§í›„**:
{poster_md if poster_md else "(í¬ìŠ¤í„° ì´ë¯¸ì§€ ì—†ìŒ)"}

3. **ìŠ¤í† ë¦¬ ì†Œê°œ ì„¹ì…˜ ì§í›„**:
{still1_md if still1_md else "(ìŠ¤í‹¸ì»· 1 ì—†ìŒ)"}

4. **ë³¼ê±°ë¦¬/í¬ì¸íŠ¸ ì„¹ì…˜ ì§í›„**:
{still2_md if still2_md else "(ìŠ¤í‹¸ì»· 2 ì—†ìŒ)"}

5. **ì´í‰ ì„¹ì…˜ ë°”ë¡œ ì§ì „**:
{still3_md if still3_md else "(ìŠ¤í‹¸ì»· 3 ì—†ìŒ)"}

---

## ì‘í’ˆ ê¸°ë³¸ ì •ë³´
- **ì œëª©(í•œ)**: {anime.get("title_korean") or "-"}
- **ì œëª©(ì˜)**: {anime.get("title_english") or "-"}
- **ì œëª©(ì¼)**: {anime.get("title_native") or "-"}
- **ì¥ë¥´**: {", ".join(anime.get("genres") or [])}
- **íƒœê·¸**: {tags_str}
- **ì œì‘ì‚¬**: {studios_str}
- **ë°©ì˜ì¼**: {tmdb_air_date or "2026ë…„ ë°©ì˜"}
- **ë°©ì˜êµ­**: {tmdb_networks or "ì¼ë³¸"}
- **AniList í‰ì **: {anime.get("average_score") or "-"}/100
- **TMDB í‰ì **: {f"{tmdb_vote:.1f}/10 ({tmdb_vote_cnt:,}ëª… í‰ê°€)" if tmdb_vote else "ì •ë³´ ì—†ìŒ"}

## ì¤„ê±°ë¦¬ (AniList)
{anime.get("synopsis") or "-"}

## ì¤„ê±°ë¦¬ (TMDB í•œêµ­ì–´)
{tmdb_overview or "(TMDB í•œêµ­ì–´ ì •ë³´ ì—†ìŒ)"}

## ì£¼ìš” ìŠ¤íƒœí”„
{staff_str}

## ì£¼ìš” ìºë¦­í„° & ì„±ìš°
{chars_str}

## ê´€ë ¨ ì‘í’ˆ
{chr(10).join(f"- {r['title']} ({r['relation']}, {r['format']})" for r in anilist_details.get("relations", [])) or "ì—†ìŒ"}

## ë¹„ìŠ·í•œ ì¶”ì²œ ì‘í’ˆ
{recs_str or "ì—†ìŒ"}

## ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤
{streaming_str or "ì •ë³´ ì—†ìŒ"}

## ê³µì‹ íŠ¸ë ˆì¼ëŸ¬
{trailer_url or trailer_yt_str or "(ì—†ìŒ)"}

## YouTube PV
{yt_pv_str or "(ì—†ìŒ)"}

## Reddit íŒ¬ ë°˜ì‘ (r/anime)
{reddit_str or "(Reddit ë°ì´í„° ì—†ìŒ)"}

---

## ì‘ì„± ìš”ì²­ ì‚¬í•­

ì•„ë˜ êµ¬ì¡°ë¡œ **í’ë¶€í•˜ê³  ê¹Šì´ ìˆëŠ”** ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”:

```
# {post_title}

[ì»¤ë²„ ì´ë¯¸ì§€: {cover_md.strip() if cover_md else "ì—†ìŒ"}]

## ğŸ’¡ ë„ì…ë¶€ (2~3 ë¬¸ë‹¨)
- ì´ ì‘í’ˆì´ ì™œ ì§€ê¸ˆ í™”ì œì¸ì§€, ë¬´ì—‡ì´ íŠ¹ë³„í•œì§€
- ë…ìì˜ í¥ë¯¸ë¥¼ ìê·¹í•˜ëŠ” í›…(Hook) ë¬¸ì¥
- í•µì‹¬ ë§¤ë ¥ í•œ ì¤„ ìš”ì•½

## ğŸ“‹ ê¸°ë³¸ ì •ë³´
- ì œëª©, ì¥ë¥´, ì œì‘ì‚¬, ë°©ì˜ì¼, ì‹œì¦Œ/ì—í”¼ì†Œë“œ ì •ë³´
- AniList/TMDB í‰ì  ë¹„êµ
- ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤ ì•ˆë‚´

[í¬ìŠ¤í„° ì´ë¯¸ì§€: {poster_md.strip() if poster_md else "ì—†ìŒ"}]

## ğŸ“– ìŠ¤í† ë¦¬ ì†Œê°œ (3~4 ë¬¸ë‹¨, ìŠ¤í¬ì¼ëŸ¬ ì—†ì´)
- ì„¸ê³„ê´€ ì„¤ëª… (3~5ë¬¸ì¥)
- ì£¼ì¸ê³µ ì†Œê°œ + í•µì‹¬ ê°ˆë“±
- ì´ì „ ì‹œì¦Œ/ì›ì‘ê³¼ì˜ ì—°ê²° (í•´ë‹¹ ì‹œ)
- ì´ë²ˆ ì‹œì¦Œ/íŒŒíŠ¸ë§Œì˜ ìƒˆë¡œìš´ ìš”ì†Œ

[ìŠ¤í‹¸ì»· 1: {still1_md.strip() if still1_md else "ì—†ìŒ"}]

## ğŸ¬ ì£¼ìš” ìºë¦­í„° & ì„±ìš°ì§„
- ì£¼ì¸ê³µê³¼ ì£¼ìš” ë“±ì¥ì¸ë¬¼ ì†Œê°œ (3~5ëª…)
- ê° ìºë¦­í„°ì˜ ì—­í• ê³¼ ë§¤ë ¥ í¬ì¸íŠ¸
- ì„±ìš° ì •ë³´ + ë‹¤ë¥¸ ëŒ€í‘œì‘

## âœ¨ ì´ ì‘í’ˆì˜ ë³¼ê±°ë¦¬ 3ê°€ì§€
ê° í•­ëª©ë§ˆë‹¤ 2~3ë¬¸ì¥ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ 

[ìŠ¤í‹¸ì»· 2: {still2_md.strip() if still2_md else "ì—†ìŒ"}]

## ğŸŒ í•´ì™¸ íŒ¬ ë°˜ì‘
- Reddit r/anime ì£¼ìš” ë°˜ì‘ ìš”ì•½
- ê¸€ë¡œë²Œ í‰ì  í•´ì„ (TMDB {tmdb_vote:.1f}/10, AniList {anime.get("average_score", 0)}/100)
- ì–´ë–¤ ì¸µì—ì„œ íŠ¹íˆ ì¸ê¸°ì¸ì§€

## ğŸ¯ ì´ëŸ° ë¶„ê»˜ ì¶”ì²œí•©ë‹ˆë‹¤
- ì¶”ì²œ ëŒ€ìƒ 3~4ê°€ì§€ (ì˜ˆ: â—‹â—‹â—‹ì„ ì¢‹ì•„í•˜ì‹ ë‹¤ë©´)
- ë¹„ìŠ·í•œ ì¶”ì²œ ì• ë‹ˆ 2~3ê°œ + ê°„ë‹¨í•œ ì´ìœ 

[ìŠ¤í‹¸ì»· 3: {still3_md.strip() if still3_md else "ì—†ìŒ"}]

## â­ ì´í‰
- ì´ ì‘í’ˆì˜ ê°•ì ê³¼ ì•½ì  ì†”ì§í•˜ê²Œ
- í˜„ ì‹œì  í‰ì  ë° ì´ìœ 
- í•œ ì¤„ ì¶”ì²œ ë©˜íŠ¸

---
í•´ì‹œíƒœê·¸ (10~15ê°œ)
```

## ì£¼ì˜ì‚¬í•­
- ë°˜ë“œì‹œ **ë§ˆí¬ë‹¤ìš´ë§Œ** ì¶œë ¥ (ì½”ë“œ ë¸”ë¡ ë˜í•‘ ì—†ì´ ë³¸ë¬¸ë§Œ)
- ì´ë¯¸ì§€ ê²½ë¡œëŠ” ìœ„ ì§€ì •ëœ ê²ƒ **ê·¸ëŒ€ë¡œ** ì‚¬ìš© (ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€)
- ì´í‰ ì„¹ì…˜ì—ëŠ” ë°˜ë“œì‹œ ë³„ì  (ì˜ˆ: â­â­â­â­â˜† 4/5)ì„ í¬í•¨
- ìŠ¤í¬ì¼ëŸ¬ ê¸ˆì§€ (ê²°ë§, ë°˜ì „ ë“±)
- í•©ë‹ˆë‹¤ì²´ ì‚¬ìš©, ì´ëª¨ì§€ ì ì ˆíˆ í™œìš©
- ì „ì²´ ë¶„ëŸ‰: **ìµœì†Œ 2,000ì ì´ìƒ** (ê¸°ì¡´ ê¸€ì˜ 3~4ë°°)
"""

    return _call_llm(prompt, max_tokens=8192)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìˆ˜ì • ëª¨ë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def revise_blog_draft(file_path: Path, instruction: str) -> str:
    """ê¸°ì¡´ ë¸”ë¡œê·¸ ê¸€(.md) ë‚´ìš©ì„ instructionì— ë§ê²Œ ìˆ˜ì •í•œ ë³¸ë¬¸ ë°˜í™˜."""
    raw = file_path.read_text(encoding="utf-8")
    prompt = f"""ë‹¤ìŒì€ ë¸”ë¡œê·¸ ê¸€ ë§ˆí¬ë‹¤ìš´ ì›ë¬¸ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§€ì‹œì— ë§ê²Œ **ìˆ˜ì •í•œ ì „ì²´ ê¸€**ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
ì½”ë“œ ë¸”ë¡ì´ë‚˜ ì„¤ëª… ì—†ì´ ìˆ˜ì •ëœ ë§ˆí¬ë‹¤ìš´ ë³¸ë¬¸ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.

## ì‚¬ìš©ì ì§€ì‹œ
{instruction}

## í˜„ì¬ ê¸€ ì›ë¬¸
---
{raw}
---

## ìš”ì²­ ì‚¬í•­
- ì§€ì‹œë¥¼ ë°˜ì˜í•´ ìˆ˜ì •í•œ **ì „ì²´** ë§ˆí¬ë‹¤ìš´ì„ ì¶œë ¥í•˜ì„¸ìš”.
- ì œëª©(# ...), ì´ë¯¸ì§€(![...](...)), ë³¸ë¬¸ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì§€ì‹œëŒ€ë¡œ ê³ ì¹˜ì„¸ìš”.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ë§Œ í•˜ì„¸ìš”."""

    return _call_llm(prompt, max_tokens=8192).strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_anime_list() -> tuple[str, int, list]:
    """seasonal_top_anime.json ë¡œë“œ."""
    if not INPUT_JSON.exists():
        raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {INPUT_JSON}")
    try:
        data = json.loads(INPUT_JSON.read_text(encoding="utf-8"))
    except (json.JSONDecodeError, OSError) as e:
        raise RuntimeError(f"JSON ë¡œë“œ ì‹¤íŒ¨: {e}") from e

    season = data.get("season", "WINTER")
    year = data.get("season_year", 2026)
    anime_list = data.get("anime") or []
    season_label = f"{year} {SEASON_KR.get(season, season)}"
    return season_label, year, anime_list


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> None:
    try:
        season_label, _year, anime_list = load_anime_list()
    except (FileNotFoundError, RuntimeError) as e:
        print(f"ì˜¤ë¥˜: {e}")
        raise

    IMAGES_DIR.mkdir(parents=True, exist_ok=True)
    POSTS_DIR.mkdir(parents=True, exist_ok=True)

    # API í‚¤ í˜„í™© ì¶œë ¥
    has_tmdb    = bool(os.environ.get("TMDB_API_KEY"))
    has_youtube = bool(os.environ.get("YOUTUBE_API_KEY"))
    has_reddit  = bool(os.environ.get("REDDIT_CLIENT_ID"))
    print(f"ğŸ“¡ API í˜„í™©: TMDB={'âœ…' if has_tmdb else 'âŒ'} | YouTube={'âœ…' if has_youtube else 'âŒ'} | Reddit={'âœ…' if has_reddit else 'âŒ(ê³µê°œAPIì‚¬ìš©)'}")
    print()

    for i, anime in enumerate(anime_list, start=1):
        title_display = (
            anime.get("title_korean")
            or anime.get("title_english")
            or anime.get("title_native")
            or "ì œëª©ì—†ìŒ"
        )
        title_en = anime.get("title_english") or anime.get("title_native") or ""
        title_native = anime.get("title_native") or ""
        slug = slugify(title_display) or f"anime_{i}"

        print(f"[{i}/{len(anime_list)}] {title_display}")
        print(f"  ğŸ” ë‹¤ì¤‘ API ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

        try:
            # 1. TMDB ê²€ìƒ‰
            print(f"  ğŸ“½ï¸  TMDB ì¡°íšŒ ì¤‘...")
            tmdb_data = tmdb_search_anime(title_en, title_native)
            if tmdb_data.get("tmdb_id"):
                print(f"  âœ… TMDB: í¬ìŠ¤í„° {len(tmdb_data.get('poster_paths', []))}ê°œ, ìŠ¤í‹¸ì»· {len(tmdb_data.get('backdrop_paths', []))}ê°œ")
            else:
                print(f"  âš ï¸  TMDB: ê²°ê³¼ ì—†ìŒ")
            time.sleep(0.3)

            # 2. AniList ìƒì„¸ (anilist_idê°€ JSONì— ì—†ìœ¼ë©´ ê±´ë„ˆëœ€)
            anilist_details = {}
            anilist_id = anime.get("anilist_id")
            if anilist_id:
                print(f"  ğŸŒ AniList ìƒì„¸ ì¡°íšŒ ì¤‘...")
                anilist_details = anilist_get_details(anilist_id)
                print(f"  âœ… AniList: ìºë¦­í„° {len(anilist_details.get('characters', []))}ëª…, íƒœê·¸ {len(anilist_details.get('tags', []))}ê°œ")
                time.sleep(0.5)
            else:
                print(f"  âš ï¸  AniList ID ì—†ìŒ â€” ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš©")

            # 3. YouTube PV ê²€ìƒ‰
            print(f"  ğŸ¬ YouTube PV ê²€ìƒ‰ ì¤‘...")
            youtube_data = youtube_search_pv(title_en, title_native)
            if youtube_data:
                print(f"  âœ… YouTube: PV {len(youtube_data)}ê°œ ë°œê²¬")
            else:
                print(f"  âš ï¸  YouTube: ê²°ê³¼ ì—†ìŒ")
            time.sleep(0.3)

            # 4. Reddit ë°˜ì‘
            print(f"  ğŸ’¬ Reddit ë°˜ì‘ ìˆ˜ì§‘ ì¤‘...")
            reddit_data = reddit_get_discussions(title_en, title_native)
            if reddit_data:
                print(f"  âœ… Reddit: ì¸ê¸° ê¸€ {len(reddit_data)}ê°œ")
            else:
                print(f"  âš ï¸  Reddit: ê²°ê³¼ ì—†ìŒ")
            time.sleep(0.3)

            # 5. ì´ë¯¸ì§€ ìˆ˜ì§‘ (5ê°œ)
            print(f"  ğŸ–¼ï¸  ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘ (ìµœëŒ€ 5ê°œ)...")
            image_paths = collect_images(anime, tmdb_data, anilist_details, slug)
            print(f"  âœ… ì´ë¯¸ì§€: {len(image_paths)}ê°œ ìˆ˜ì§‘ ({', '.join(image_paths.keys())})")

            # 6. ë¸”ë¡œê·¸ ê¸€ ìƒì„±
            print(f"  âœï¸  ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì¤‘...")
            body = generate_blog_draft(
                anime=anime,
                season_label=season_label,
                image_paths=image_paths,
                anilist_details=anilist_details,
                tmdb_data=tmdb_data,
                youtube_data=youtube_data,
                reddit_data=reddit_data,
            )

            # 7. ì €ì¥
            post_filename = f"{slug}.md"
            post_path = POSTS_DIR / post_filename
            post_path.write_text(body.strip(), encoding="utf-8")
            word_count = len(body.replace(" ", ""))
            print(f"  âœ… ì €ì¥ ì™„ë£Œ: {post_path} ({word_count:,}ì)")

        except Exception as e:
            print(f"  âŒ ì‹¤íŒ¨: {e}")
            raise

        print()

    print(f"ğŸ‰ ì™„ë£Œ: {len(anime_list)}ê°œ ê¸€ ìƒì„±")
    print(f"   ì´ë¯¸ì§€: {IMAGES_DIR}")
    print(f"   ê¸€: {POSTS_DIR}")


def run_revise_mode(revise_path: Path, instruction: str) -> None:
    """--revise <íŒŒì¼> --instruction <ì§€ì‹œ> ëª¨ë“œ."""
    if not revise_path.exists():
        print(f"ì˜¤ë¥˜: íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {revise_path}", file=sys.stderr)
        sys.exit(1)
    if not instruction.strip():
        print("ì˜¤ë¥˜: --instruction ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", file=sys.stderr)
        sys.exit(1)
    try:
        revised = revise_blog_draft(revise_path, instruction)
        revise_path.write_text(revised, encoding="utf-8")
        print(f"ìˆ˜ì • ì™„ë£Œ: {revise_path}")
    except Exception as e:
        print(f"ì˜¤ë¥˜: {e}", file=sys.stderr)
        raise


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="ì• ë‹ˆ ë¸”ë¡œê·¸ ê¸€ ìƒì„±(ê¸°ë³¸) ë˜ëŠ” ê¸°ì¡´ ê¸€ ìˆ˜ì •(--revise)"
    )
    parser.add_argument(
        "--revise", type=Path, metavar="PATH",
        help="ìˆ˜ì •í•  .md íŒŒì¼ ê²½ë¡œ (--instructionê³¼ í•¨ê»˜ ì‚¬ìš©)",
    )
    parser.add_argument(
        "--instruction", type=str, default="", metavar="TEXT",
        help="ìˆ˜ì • ì§€ì‹œë¬¸ (--reviseì™€ í•¨ê»˜ ì‚¬ìš©)",
    )
    args = parser.parse_args()

    if args.revise is not None:
        run_revise_mode(args.revise, args.instruction or "")
    else:
        main()
